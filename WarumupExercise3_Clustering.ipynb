{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed discussion on TF-IDF and K-Means Clustering can be found in Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF (Refer Appendix as well)\n",
    "\n",
    "To understand TF-IDF, we need to understand two terminologies first:\n",
    "\n",
    "- Term-Frequency: The intuition to use this is that documents of particular type will have similar words. Therefore, if certain words that occur frequently will indicate certain topic. Terms that occur highly or terms which are quite rare will be pruned off just like stop-words are removed while doing text preprocessing.\n",
    "\n",
    "- Inverse Document Frequency: The limitation of TF is that it is not effective in terms of term-weighting, where selected terms will have the same weights all the times. Say some words have discriminative power to distinguish a particular group of documents, at that time TF would be ineffective and hence IDF comes for the salvation. The value of IDF will be high for rare terms and low for highly frequent ones. \n",
    "\n",
    "TF-IDF combines the advantages of both above methods. It assigns greater values to terms that occur frequently in a small set of documents, thus having more discriminative power. This value will be penalized when the term occurs in more documents. Lowest value is given to the terms that occur in all documents. Hence, TF-IDF yields better clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets follow the article to understand how k-means clustering is done: \n",
    "\n",
    "https://medium.com/@rohithramesh1991/unsupervised-text-clustering-using-natural-language-processing-nlp-1a8bc18b048d\n",
    "\n",
    "Lets note how to find optimal k referring the above article\n",
    "\n",
    "Lets try implementing our own clustering on the Social Media updates by Mr. Jack (Don't worry about n-grams yet, we can do it in our project as we can have larger corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Today its raining so I can't go out to play\",\n",
    "\"I like ice hockey, football and basketball and tennis is my favorite\",\n",
    "\"I want to go to party tonight. Let me know who wants to join me\",\n",
    "\"Who knows how to play cricket? Let's try some new sports as we have holidays now.\",\n",
    "\"All work, no plays makes me a dull boy... time to chill\",\n",
    "\"Who is watching English Premium League tonight  ?\",\n",
    "\"I want to go to Lapland for my winter holidays.\",\n",
    "\"Christmas holidays was fun. I enjoyed playing indoor sports with my grandfather.\",\n",
    "\"Ice cream vs Ice hockey. Now its summer :D\",\n",
    "\"Summer finished and I didn't play anything all the holidays. #CORONA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TF-IDF vectorizer and remove the stop words\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Because of the parameter str_pattern of TfidfVectorizer, the punctuations are completely removed. (Check out the documentation)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)  # This is our new vectorizer\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=2, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "assigned_k = 2  \n",
    "kmeans_model = KMeans(n_clusters=assigned_k, init='k-means++', max_iter=100, n_init=1)\n",
    "#We are fitting unsupervised model (i.e. K-means clustering to our data(vectorized corpus))\n",
    "kmeans_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets try printing various values of k using elbow method\n",
    "\n",
    "#Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check these documentation to understand what the following snippet does:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html (Specially the 'cluster_center_' attribute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = kmeans_model.cluster_centers_\n",
    "terms = vectorizer.get_feature_names()\n",
    "#order_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may want to sort according to the highest values. Lets also check the following documentation before \n",
    "\n",
    "- https://numpy.org/doc/stable/reference/generated/numpy.argsort.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What will the following snippet do ?\n",
    "\n",
    "# x = np.array([[3, 1, 2],[1,2,3]])\n",
    "# x.argsort()[:, ::-1]  #Originally it was ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = order_centroids.argsort()[:, ::-1] # The expression in square brace is to slice a numpy 2d-array. The first argument is row. So here we select all rows and then columns in reversed order\n",
    "\n",
    "#Now lets check the centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basketball',\n",
       " 'boy',\n",
       " 'chill',\n",
       " 'christmas',\n",
       " 'corona',\n",
       " 'cream',\n",
       " 'cricket',\n",
       " 'didn',\n",
       " 'dull',\n",
       " 'english',\n",
       " 'enjoyed',\n",
       " 'favorite',\n",
       " 'finished',\n",
       " 'football',\n",
       " 'fun',\n",
       " 'grandfather',\n",
       " 'hockey',\n",
       " 'holidays',\n",
       " 'ice',\n",
       " 'indoor',\n",
       " 'join',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'lapland',\n",
       " 'league',\n",
       " 'let',\n",
       " 'like',\n",
       " 'makes',\n",
       " 'new',\n",
       " 'party',\n",
       " 'play',\n",
       " 'playing',\n",
       " 'plays',\n",
       " 'premium',\n",
       " 'raining',\n",
       " 'sports',\n",
       " 'summer',\n",
       " 'tennis',\n",
       " 'time',\n",
       " 'today',\n",
       " 'tonight',\n",
       " 'try',\n",
       " 'vs',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'watching',\n",
       " 'winter',\n",
       " 'work']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(terms)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "tonight \n",
      "\n",
      "watching \n",
      "\n",
      "league \n",
      "\n",
      "premium \n",
      "\n",
      "english \n",
      "\n",
      "join \n",
      "\n",
      "wants \n",
      "\n",
      "party \n",
      "\n",
      "know \n",
      "\n",
      "basketball \n",
      "\n",
      "like \n",
      "\n",
      "favorite \n",
      "\n",
      "football \n",
      "\n",
      "tennis \n",
      "\n",
      "dull \n",
      "\n",
      "chill \n",
      "\n",
      "boy \n",
      "\n",
      "makes \n",
      "\n",
      "work \n",
      "\n",
      "time \n",
      "\n",
      "Cluster 1:\n",
      "holidays \n",
      "\n",
      "play \n",
      "\n",
      "summer \n",
      "\n",
      "ice \n",
      "\n",
      "sports \n",
      "\n",
      "today \n",
      "\n",
      "raining \n",
      "\n",
      "lapland \n",
      "\n",
      "winter \n",
      "\n",
      "want \n",
      "\n",
      "corona \n",
      "\n",
      "didn \n",
      "\n",
      "finished \n",
      "\n",
      "vs \n",
      "\n",
      "cream \n",
      "\n",
      "try \n",
      "\n",
      "cricket \n",
      "\n",
      "knows \n",
      "\n",
      "new \n",
      "\n",
      "enjoyed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets print the words in each cluster\n",
    "for i in range(assigned_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for index in order_centroids[i, :20]:  #We are checking only the first 20 of each cluster\n",
    "        print('%s' % terms[index],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print('Prediction')\n",
    "X = vectorizer.transform(['Nothing is easy in football.']) #Testing our model for a new test case\n",
    "\n",
    "predicted = kmeans_model.predict(X)  \n",
    "print(predicted) #See which cluster the test sentence X belongs to. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Why do you think it predicted in the 0 cluster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to analyze does the clustering makes sense ? As a further task, try increasing the size of your corpus. So instead of taking the corpus from Mr Jack's social Media, you can now try writing 15 sentences on your favorite sport. Use cohorent sentences because our corpus will still be quite less. Feel free to assign number of clusters and play around with various parameters. \n",
    "\n",
    "- Use some other methods like CountVectorizer or Bag-of-Words for vectorizing our corpus. Follow all the text preprocessing that we have done in our earlier tasks. If you go with CountVectorizer, you wont have to bother about punctuation and lowering letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
