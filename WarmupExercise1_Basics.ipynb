{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our next exercise we are going to extract our own data and make our own corpus (Collection of texts). For this exercise, let's get it from Project Gutenberg (https://www.gutenberg.org/). The Natural Language Toolkit (NLTK) suite has a lots of libraries that makes learning Natural Language Processing easy and fun. It also has small section of texts from the project gutenberg we are going to use for our purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\tranq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See the amount of libraries we are using from the toolkit!\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('genesis')\n",
    "nltk.download('inaugural')\n",
    "nltk.download('nps_chat')\n",
    "nltk.download('webtext')\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This documentation can be checked (1.1 only):\n",
    "https://www.nltk.org/book/ch02.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#from nltk.book import *\n",
    "from nltk.corpus import gutenberg\n",
    "words = gutenberg.words('shakespeare-hamlet.txt')  #To get the words \n",
    "sentences = gutenberg.sents('shakespeare-hamlet.txt') # Get the sentences\n",
    "sample = gutenberg.raw('shakespeare-hamlet.txt')  #To get the raw text from the read file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to answer what raw() method does, if you have followed the nltk reference above. It is quite obvious what .words() and .sents() do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see how many books are there. For this we call the fileids() identifier in the corpus.\n",
    "gutenberg.fileids()\n",
    "# gutenberg.fileids()[-3] #We just took one of the 18 books. counting 3 from the last gives us shakespeare-hamlet.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that only one command can be executed in a cell. If you want to run a command, just comment out (delete the hash and indent correctly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try playing around with each variables we defined so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check what's in the sample. Remember to comment out the required code/variable/keywords/variables in the cell .\n",
    "\n",
    "#sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the final 50 words\n",
    "\n",
    "#words[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words do we have ?\n",
    "\n",
    "#len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just see the final 10 sentences\n",
    "\n",
    "#sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You', 'come', 'most', 'carefully', 'vpon', 'your', 'houre']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the 16th sentence (remember count starts from 0). Feel free to check any sentence. You can check multiple sentences as well if you specify start and end sentence.\n",
    "\n",
    "sentences[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets perform sentence tokenization. Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms, sentences, etc. Each of these smaller units are called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[The Tragedie of Hamlet by William Shakespeare 1599]\\n\\n\\nActus Primus.',\n",
       " 'Scoena Prima.',\n",
       " 'Enter Barnardo and Francisco two Centinels.',\n",
       " 'Barnardo.',\n",
       " \"Who's there?\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokenize = sent_tokenize (sample)\n",
    "#Lets check first 5 sentences separately in one output\n",
    "tokenize[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now check the number of paragraphs (Here we check first 10 only). As we have described above, paragraph can also be tokenized. \n",
    "#Uncomment all the following code lines in one go by selecting all and press ctrl+/. You can comment back in the same way.\n",
    "\n",
    "# len(gutenberg.paras('shakespeare-hamlet.txt'))\n",
    "\n",
    "# for para in range(10):\n",
    "#     print(tokenize[para])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next warmup exercise, we will use Lambda expression for processing text which is very powerful and easy way to do a lot of stuffs in one go. \n",
    "\n",
    "https://www.w3schools.com/python/python_lambda.asp\n",
    "\n",
    "Here we do it in detail instead of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observe the output\n",
    "\n",
    "#sample.split(' ') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets explore regular expression. Specially look at how we do substitution. \n",
    "\n",
    "https://www.w3schools.com/python/python_regex.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'tragedie',\n",
       " 'of',\n",
       " 'hamlet',\n",
       " 'by',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " '1599actus',\n",
       " 'primus',\n",
       " 'scoena',\n",
       " 'primaenter',\n",
       " 'barnardo',\n",
       " 'and',\n",
       " 'francisco',\n",
       " 'two',\n",
       " 'centinels',\n",
       " 'barnardo',\n",
       " 'whos',\n",
       " 'there',\n",
       " 'fran',\n",
       " 'nay',\n",
       " 'answer',\n",
       " 'me',\n",
       " 'stand',\n",
       " 'vnfoldyour',\n",
       " 'selfe',\n",
       " 'bar',\n",
       " 'long',\n",
       " 'liue',\n",
       " 'the',\n",
       " 'king',\n",
       " 'fran',\n",
       " 'barnardo',\n",
       " 'bar',\n",
       " 'he',\n",
       " 'fran',\n",
       " 'you',\n",
       " 'come',\n",
       " 'most',\n",
       " 'carefully',\n",
       " 'vpon',\n",
       " 'your',\n",
       " 'houre',\n",
       " 'bar',\n",
       " 'tis',\n",
       " 'now',\n",
       " 'strook',\n",
       " 'twelue',\n",
       " 'get',\n",
       " 'thee']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean the text. Tokenize it, lower-case it, remove unnecessary ....\n",
    "import re\n",
    "sample_split = []\n",
    "for st in sample.strip().lower().split(' '):  ##?? why so much in one line ? What's going on ?\n",
    "    st = re.sub(r\"[^a-zA-Z0-9]+\", '', st)  #What will this regex do ? Feel free to do extra cleaning and play with regex. But first lets proceed with this much\n",
    "        \n",
    "    sample_split.append(st)\n",
    "    #print(st)\n",
    "sample_split\n",
    "\n",
    "while '' in sample_split:   #The extra '\\n' will be '' so we need to remove it now\n",
    "    sample_split.remove('')\n",
    "    \n",
    "sample_split[:50] #Just first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tragedie',\n",
       " 'hamlet',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " '1599actus',\n",
       " 'primus',\n",
       " 'scoena',\n",
       " 'primaenter',\n",
       " 'barnardo',\n",
       " 'francisco',\n",
       " 'two',\n",
       " 'centinels',\n",
       " 'barnardo',\n",
       " 'whos',\n",
       " 'fran',\n",
       " 'nay',\n",
       " 'answer',\n",
       " 'stand',\n",
       " 'vnfoldyour',\n",
       " 'selfe',\n",
       " 'bar',\n",
       " 'long',\n",
       " 'liue',\n",
       " 'king',\n",
       " 'fran',\n",
       " 'barnardo',\n",
       " 'bar',\n",
       " 'fran',\n",
       " 'come',\n",
       " 'carefully',\n",
       " 'vpon',\n",
       " 'houre',\n",
       " 'bar',\n",
       " 'tis',\n",
       " 'strook',\n",
       " 'twelue',\n",
       " 'get',\n",
       " 'thee',\n",
       " 'bed',\n",
       " 'francisco',\n",
       " 'fran',\n",
       " 'releefe',\n",
       " 'much',\n",
       " 'thankes',\n",
       " 'tis',\n",
       " 'bitter',\n",
       " 'coldand',\n",
       " 'sicke',\n",
       " 'heart',\n",
       " 'barn',\n",
       " 'haue',\n",
       " 'quiet',\n",
       " 'guard',\n",
       " 'fran',\n",
       " 'mouse',\n",
       " 'stirring',\n",
       " 'barn',\n",
       " 'well',\n",
       " 'goodnight',\n",
       " 'meet',\n",
       " 'horatio',\n",
       " 'andmarcellus',\n",
       " 'riuals',\n",
       " 'watch',\n",
       " 'bid',\n",
       " 'make',\n",
       " 'hastenter',\n",
       " 'horatio',\n",
       " 'marcellus',\n",
       " 'fran',\n",
       " 'thinke',\n",
       " 'heare',\n",
       " 'stand',\n",
       " 'whos',\n",
       " 'hor',\n",
       " 'friends',\n",
       " 'ground',\n",
       " 'mar',\n",
       " 'leigemen',\n",
       " 'dane',\n",
       " 'fran',\n",
       " 'giue',\n",
       " 'good',\n",
       " 'night',\n",
       " 'mar',\n",
       " 'farwel',\n",
       " 'honest',\n",
       " 'soldier',\n",
       " 'hath',\n",
       " 'relieud',\n",
       " 'fra',\n",
       " 'barnardo',\n",
       " 'place',\n",
       " 'giue',\n",
       " 'goodnightexit',\n",
       " 'fran',\n",
       " 'mar',\n",
       " 'holla',\n",
       " 'barnardo',\n",
       " 'bar',\n",
       " 'say',\n",
       " 'horatio',\n",
       " 'hor',\n",
       " 'peece',\n",
       " 'bar',\n",
       " 'welcome',\n",
       " 'horatio',\n",
       " 'welcome',\n",
       " 'good',\n",
       " 'marcellus',\n",
       " 'mar',\n",
       " 'thing',\n",
       " 'appeard',\n",
       " 'againe',\n",
       " 'night',\n",
       " 'bar',\n",
       " 'haue',\n",
       " 'seene',\n",
       " 'nothing',\n",
       " 'mar',\n",
       " 'horatio',\n",
       " 'saies',\n",
       " 'tis',\n",
       " 'fantasieand',\n",
       " 'let',\n",
       " 'beleefe',\n",
       " 'take',\n",
       " 'hold',\n",
       " 'himtouching',\n",
       " 'dreaded',\n",
       " 'sight',\n",
       " 'twice',\n",
       " 'seene',\n",
       " 'vstherefore',\n",
       " 'haue',\n",
       " 'intreated',\n",
       " 'alongwith',\n",
       " 'vs',\n",
       " 'watch',\n",
       " 'minutes',\n",
       " 'nightthat',\n",
       " 'againe',\n",
       " 'apparition',\n",
       " 'comehe',\n",
       " 'may',\n",
       " 'approue',\n",
       " 'eyes',\n",
       " 'speake',\n",
       " 'hor',\n",
       " 'tush']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove stopwords now\n",
    "words_nostopwords = []\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#What is stop words and corpus, just search in wikipedia\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for word in sample_split:\n",
    "    if word not in stop_words:\n",
    "        words_nostopwords.append(word)\n",
    "\n",
    "words_nostopwords[:150]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization:\n",
    "According to Wikipedia:\n",
    "\n",
    "Lemmatisation in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's do Lemmatization. \n",
    "words_lemmatized = []\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for word in words_nostopwords:\n",
    "    words_lemmatized.append(wnl.lemmatize(word))\n",
    "    \n",
    "#words_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of preparations to get sample in desired state so that now we are ready to show some magics and tricks are ready to be displayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the count of words and see the ones that occur more frequently. For that we use FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_sample1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f95dc73ce119>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfreq_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_sample1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_sample1' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq_dist = FreqDist(new_sample1)\n",
    "print(freq_dist)\n",
    "\n",
    "freq_dist.most_common(20)  # Understanding Shakespeare's Vocabulary set ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "freq_dist.plot(20,cumulative=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = nltk.word_tokenize(sample)   # tokenize it\n",
    "mytext = nltk.Text(tokens)         # turn text into a NLTK Text object\n",
    "mytext[:5]\n",
    "#type(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using for loop instead of list slicing, as used above. \n",
    "\n",
    "for word in range(5):\n",
    "    print(mytext[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now lets try dispersion plot (For this we will need special object of NLTK nltk.text.Text, which we have already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let me test my favorite words in Shakespeare's play\n",
    "\n",
    "mytext.dispersion_plot(['hello','good','No','fine','ok','yes'])  ##Recalling mytext .... where did we assign it ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mytext) #Why am i checking it here ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech,based on both its definition and its context—i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "[Wikipedia]\n",
    "\n",
    "Check out the first example in this: \n",
    "\n",
    "https://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feel free to try pos tagging following the example in the link above. \n",
    "\n",
    "#Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('universal_tagset')\n",
    "#Feel free to slice the mytext object. \n",
    "lst = mytext[:10]\n",
    "pos_tag = nltk.pos_tag(lst,tagset = 'universal')  #Try without parameter tagset\n",
    "pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named-entity recognition is a subtask of information extraction that seeks to locate and classify named entity mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. [Wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right now there is an error due to GUI. Will be fixed soon\n",
    "\n",
    "from nltk import ne_chunk #Chunking adds more structure \n",
    "\n",
    "named_entity = nltk.ne_chunk(pos_tag)\n",
    "named_entity.draw() #Magic! Something like below should appear\n",
    "\n",
    "#Close the image that appears if you want to proceed to next cells"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try to print only the names\n",
    "for x in str(named_entity).split('\\n'):\n",
    "    if '/NN' in x:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the ] and [ also being tagged as Nouns. So Let's try with our cleaned tokenized text. GPE here stands for Geo-Political Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(new_sample1[:10],tagset = 'universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feel free to add cells (Click the '+' sign) and do the stuffs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Word2Vec](http://radimrehurek.com/gensim/models/word2vec.html) uses neural networks to analyze words in a corpus by using the contexts of words. It takes as its input a large corpus of text, and maps unique words to a vector space, such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
    "Word2Vec finds words that are used in combination with other words. So orange and juice may have a high similarity.\n",
    "You can see here the context of one word (pain) for two different corpora.\n",
    "This uses the popular gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "hamlet_vect = Word2Vec(sentences)\n",
    "blake_sentence = gutenberg.sents('blake-poems.txt')\n",
    "blakes_vect = Word2Vec(blake_sentence)  #We pass corpus as input\n",
    "print(hamlet_vect.wv.most_similar('thee', topn=6))\n",
    "print('\\n')\n",
    "print(blakes_vect.wv.most_similar('thee', topn=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So why do we need to vectorize the words then ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lets try preprocessing the corpus and then see the result. Below with Topic modeling, this has been done. But before going there give it a try by yourself ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is unsupervised learning method that uses probabilistic model to identify group of topics. It discovers abstract topics that occur in a collection of documents. (We can explore its inner workings in our project word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus is simply collection of documents and each document is a collection of words\n",
    "corpus = blake_sentence[5:15] #Feel free to slice the tokenized sentences\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the corpus into a matrix representation. \n",
    "First of all we have to create term dictionary of the corpus. Then we convert the dictionary into document-term matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora # Corpora is plural of corpus\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "#print(dictionary)\n",
    "\n",
    "# Bag-of-words or bow tells you what words occur in the document\n",
    "DocTerm_matrix = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "Lda_object = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_1 = Lda_object(DocTerm_matrix, num_topics=2, id2word = dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model_1.print_topics(num_topics=2, num_words=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks stupid. Lets try after preprocessing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets convert list into string so that we can do preprocessing with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a list s and keeps joining every word in them\n",
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \" \"  \n",
    "\n",
    "    # return string   \n",
    "    return str1.join(s)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listToString(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "no_punct = set(string.punctuation)\n",
    "lemma_obj = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(document):\n",
    "    #Each document is a list. We need to convert it to string before we can run any string functions on them.\n",
    "    no_stopwords = \" \".join([i for i in listToString(document).lower().split() if i not in stopwords])\n",
    "    no_punctuation = ''.join(ch for ch in no_stopwords if ch not in no_punct)\n",
    "    lemmatized = \" \".join(lemma_obj.lemmatize(word) for word in no_punctuation.split())\n",
    "    return lemmatized\n",
    "\n",
    "final_corpus = [preprocess(document).split() for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(final_corpus)\n",
    "\n",
    "DocTerm_matrix = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "Lda_object = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "lda_model_2 = Lda_object(DocTerm_matrix, num_topics=2, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model_2.print_topics(num_topics=2, num_words=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very poetic! Of course it would be if Shakespeare is the writer. As an assignment let's try with some other text books in gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here is what we learnt in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try some other corpus. Feel free to add cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text Pre-processing\n",
    "- Frequency Distribution Plot\n",
    "- Lexical Dispersion Plot\n",
    "- POS Tagging and NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
